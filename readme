Nice — solid choice. I’ll produce a complete, ready-to-run project plan without the Pi AI Hat, that trains an object detector to classify tomatoes into “ready (ripe) / not ready (unripe) / spoilt (bad)”, then deploys it on a Raspberry Pi 5 to drive a robotic arm (Arduino/servo) for sorting. At the end I’ll give you a single, copy-pasteable prompt for Cyusor AI (so the assistant can help you generate code, train, and deploy).

I keep this practical and actionable — commands, folder structure, model choices, training hyperparameters, deployment code snippets, hardware + wiring notes, test plan, metrics and a timeline.

1 — Project summary (one line)

AI-Powered Tomato Sorter: Train a YOLO-based detector to classify tomatoes as Ready / Not Ready / Spoilt, run inference on Raspberry Pi 5 (CPU/ONNX/TFLite), and command an Arduino-driven robot arm to sort tomatoes into bins.

2 — Key decisions & rationale

Model family: YOLOv8 (Ultralytics) or YOLOv6-nano. I recommend YOLOv8n (Nano) for easiest training and smooth export to ONNX/TFLite, while keeping good accuracy and small size.

Training machine: Your Linux laptop (with GPU if available). If not, use Google Colab (GPU runtime).

Pi deployment: Convert to ONNX or TFLite and run inference on Raspberry Pi 5 using OpenCV DNN (ONNX) or tflite-runtime (TFLite). Do post-training quantization to speed up CPU inference.

Control: Raspberry Pi handles vision + decision logic; Arduino (or ESP32/STM32) handles PWM servo control and low-level safety. Pi ↔ Arduino via serial (USB or UART).

3 — Data: collect & annotate (practical plan)
A. Dataset composition (target)

Total images goal: 2,000 – 4,000 images (mix of lab & farm/market lighting).

Ready (ripe): 800 – 1,600 images

Not ready (unripe): 600 – 1,200 images

Spoilt (rotten/blemished): 400 – 800 images

Splits: 70% train / 15% val / 15% test.

B. Capture guidelines

Use the Pi camera or smartphone. Vary:

Lighting (sunny, shade, indoor fluorescent).

Backgrounds (crate, conveyor belt, hands, leaves).

Distances and angles (top-down, 30° oblique).

Single fruit and small clusters/overlaps.

Include occlusions and partially visible fruit to improve robustness.

C. Annotation format & tools

Use LabelImg or Roboflow for bounding boxes. Save as YOLO Darknet format (txt per image: class x_center y_center width height normalized).

Class IDs mapping:

0 — not_ready

1 — ready

2 — spoilt

Folder structure:

tomato_dataset/
  images/
    train/
    val/
    test/
  labels/
    train/
    val/
    test/
  data.yaml


Example data.yaml (Ultralytics format):

path: /full/path/to/tomato_dataset
train: images/train
val: images/val
test: images/test
names: ['not_ready', 'ready', 'spoilt']
nc: 3

D. Augmentation recommendations

Random brightness & contrast (±25%).

Random horizontal flip, small rotation (±15°).

Random scale and crop (0.8–1.2).

Gaussian blur occasionally.

Color jitter (for different ripeness hues).
Use either Roboflow augmentation pipelines or the built-in augmentation in YOLOv8.

4 — Training (step-by-step using YOLOv8)
A. Setup (on Linux laptop)
# create venv
python3 -m venv yolov8-env
source yolov8-env/bin/activate

pip install -U pip
pip install ultralytics  # includes training, export, inference helpers

B. Train command (example)

Use YOLOv8 nano (yolov8n.pt) as base checkpoint (transfer learning).

# from project root where data.yaml exists
from ultralytics import YOLO
# or use CLI:
yolo detect train model=yolov8n.pt data=/full/path/to/data.yaml epochs=100 imgsz=640 batch=16


If GPU present, adjust batch accordingly. If no GPU use smaller batch (e.g., 8).

Recommended hyperparams:

epochs: 80–120 (monitor val loss; early stop if overfitting)

imgsz: 640 (or 512 if memory constrained)

optimizer: SGD or AdamW (YOLO default is fine)

lr: default is OK; reduce if unstable

C. Monitoring & evaluation

Monitor mAP@0.5, precision, recall, and loss.

Save best best.pt.

After training, run yolo val model=best.pt data=data.yaml to get final metrics.

5 — Model export & optimization for Pi 5
A. Export to ONNX and TFLite (Ultralytics makes it easy)
# export to onnx and tflite
yolo export model=best.pt format=onnx  # produces best.onnx
yolo export model=best.pt format=tflite  # produces best.tflite

B. Quantization (TFLite recommended for CPU speed)

Use post-training dynamic range quantization or full integer quantization (if you have a representative dataset).

With ultralytics exported TFLite, apply tflite quantizer or use TensorFlow converter:

# example: full integer quantization (representative dataset generator needed)
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model('saved_model_dir')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representative_data_gen():
    for image_path in representative_images:
        img = preprocess(image_path)  # resize, normalize
        yield [img]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
tflite_quant = converter.convert()
open('model_int8.tflite','wb').write(tflite_quant)


If using ONNX + OpenCV DNN on Pi, try FP16 or use ONNX Runtime with OpenVINO if available.

C. Benchmark on Pi 5

Copy ONNX/TFLite to Pi and benchmark inference time (ms) per frame using a test script. Target: ≥ 3–10 FPS depending on model size and quantization.

6 — Raspberry Pi inference script (ONNX via OpenCV DNN example)

Save as inference_pi.py on Pi 5:

import cv2
import numpy as np
import serial, time

# Serial to Arduino
ser = serial.Serial('/dev/ttyUSB0', 115200, timeout=1)
time.sleep(2)

model_path = 'best.onnx'
net = cv2.dnn.readNetFromONNX(model_path)
net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)

cap = cv2.VideoCapture(0)  # PiCamera via v4l2 or USB cam

def preprocess(frame, size=640):
    h, w = frame.shape[:2]
    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (size,size), swapRB=True, crop=False)
    return blob, w, h

while True:
    ret, frame = cap.read()
    if not ret: break
    blob, w, h = preprocess(frame, 640)
    net.setInput(blob)
    preds = net.forward()  # YOLO ONNX output format might require postprocessing

    # ---- Minimal YOLO postprocessing (example; match your model's output) ----
    # Use Ultralytics export format or adapt to detections returned.
    # Here assume preds is [N, 6] rows: x1,y1,x2,y2,score,class
    # For many ONNX exports you need to decode anchors; consider using ultralytics export that outputs boxes.

    # Example: pretend preds is already decoded:
    detections = []  # fill with (x1,y1,x2,y2,score,cls)
    for x1,y1,x2,y2,score,cls in detections:
        cx = (x1 + x2)/2.0 / w
        cy = (y1 + y2)/2.0 / h
        # determine bin by class or by x-position
        ser.write(f'MOVE {cx:.3f} {cy:.3f} {int(cls)}\n'.encode())

    # Visualization for debugging
    cv2.imshow('frame', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()


Note: The exact ONNX output decoding depends on how you exported YOLO. Ultralytics exports with a simplified postprocess option; adjust code accordingly (Ultralytics provides an exported Python inference wrapper). The main point: detect → compute object center → send command to Arduino.

7 — Mapping detections → arm coordinates
A. Setup options

Top-down fixed camera above workspace: easiest — pixel X → arm X on conveyor/bins (calibration linear mapping). No depth needed.

Side camera + depth sensor (recommended for free placement): use Intel RealSense / stereo / ToF to get 3D coordinates. More complex.

B. Simple calibration (top-down)

Place checkerboard or calibration marks.

Compute homography or affine mapping from pixel coordinates to real-world coordinates (cm).

Use linear calibration: pick 4 known points, compute transformation matrix.

Example:

# Given pixel (px, py), transform to world (wx, wy) using cv2.getPerspectiveTransform() or cv2.findHomography()

C. Inverse Kinematics (brief)

For a 5-DOF arm, use planar IK for shoulder & elbow (x,y -> theta1, theta2), base yaw from atan2(x,y), wrist alignment to keep end-effector level, and a dedicated gripper servo. Use existing libs (TinyIK, ikpy) or code simple 2-link trigonometric solver for the planar joints.

Send joint angles to Arduino; Arduino interpolates to move servos smoothly.

8 — Arduino side (servo controller example)

Arduino listens to serial commands like ANGLE 90 45 120 (joint angles):

#include <Servo.h>

Servo s1, s2, s3;
void setup(){
  Serial.begin(115200);
  s1.attach(3); s2.attach(5); s3.attach(6);
}
void loop(){
  if(Serial.available()){
    String line = Serial.readStringUntil('\n');
    // parse, e.g., "ANGLE 90 45 120"
    if(line.startsWith("ANGLE")){
      int a1, a2, a3;
      sscanf(line.c_str(), "ANGLE %d %d %d", &a1, &a2, &a3);
      s1.write(a1); s2.write(a2); s3.write(a3);
    }
    // emergency stop
    if(line.startsWith("STOP")) { /* cut PWM or go to safe pose */ }
  }
}

9 — Evaluation & metrics (how to grade success)

mAP@0.5 on test set ≥ 0.75 as a target (adjustable).

Precision and recall for each class: prioritize spoilt detection (high recall to avoid bad fruit leaving).

End-to-end metrics:

Sorting accuracy = (# correctly sorted fruits) / (total tested). Aim ≥ 85% for demo.

Average inference latency (ms). Aim for ≤ 300 ms per frame on Pi after quantization.

Pick success rate (mechanical) over N trials (e.g., 30 picks) — aim ≥ 80%.

10 — Risks & mitigations

Lighting sensitivity: Use controlled lighting or augment dataset heavily. Consider an enclosed light box for consistent color-based ripeness detection.

Ambiguous ripeness: Combine color + texture features; consider adding multispectral or NIR if budget allows.

Servo strength: Choose servos rated for payload; use gripper with soft pads to avoid squashing fruit.

Pi compute limits: Use small model + TFLite quantization or offload heavy training to laptop/Colab.

11 — Timeline (12 weeks — semester-ready)

Weeks 1–2: Project setup & dataset collection plan

Buy parts, prepare camera mount, start photographing tomatoes in varied conditions.

Set up annotation workflow (LabelImg / Roboflow).

Weeks 3–4: Annotations + early model experiments

Label 800+ images; run a small training to validate classes.

Try classical OpenCV color thresholding baseline.

Weeks 5–6: Full training & evaluation

Train YOLOv8n with full dataset; tune augmentations/hyperparams.

Export best weights.

Weeks 7–8: Optimize for Pi & integration tests

Export ONNX/TFLite, quantize, benchmark on Pi.

Implement Pi inference script and simple serial messaging to Arduino.

Weeks 9–10: Build arm prototype + calibration

3D print arm parts, assemble servos and gripper, program Arduino IK stub.

Calibrate pixel -> world mapping.

Weeks 11–12: Testing, field trials, write report & presentation

Run batch tests (N≥50 fruits), collect metrics, refine.

Prepare 2–3 minute demo video and report.

12 — Deliverables you can submit

Data: annotated dataset sample + data.yaml (link or folder).

Trained model: best.pt and exported best.onnx or best.tflite.

Pi inference script and Arduino firmware.

Final report: methodology, metrics, demo pictures/video.

Short video of sorting demo (phone recording works).

13 — Example commands cheat-sheet

Clone repo, train, export (Ultralytics CLI)

pip install ultralytics
# train
yolo detect train model=yolov8n.pt data=/path/to/data.yaml epochs=100 imgsz=640 batch=16

# export
yolo export model=runs/detect/train/weights/best.pt format=onnx,tflite


Run val:

yolo val model=runs/detect/train/weights/best.pt data=/path/to/data.yaml


Run inference locally:

yolo detect predict model=best.pt source=images/test/ --save

14 — Practical tips & small improvements

Use a light box for controlled demo lighting to maximize accuracy when you show your project.

Start your demo with single-fruit pick from conveyor (easier than heap picking).

For tricky classification between ripe & unripe, add a color histogram feature as a fallback or input to a small classifier if YOLO confuses classes.

Log inference outputs to CSV for later analysis (timestamp, class, bbox, confidence, action taken).

15 — Prompt for Cyusor AI (copy & paste)

Use this prompt to ask Cyusor (or similar assistant) for a step-by-step code + training + deployment pipeline. It’s explicit so the AI can produce code, scripts and files you can run.

You are an expert embedded vision/robotics engineer. I need a complete, runnable pipeline for a final-year project: an AI tomato sorter (classes: "not_ready", "ready", "spoilt"). I will train on my Linux laptop (or Colab), deploy on a Raspberry Pi 5 (no Hailo Hat), and control an Arduino-driven 5-DOF robotic arm (base, shoulder, elbow, wrist, gripper) that sorts tomatoes into 3 bins with distance-aware placement.

Please produce the following, with exact commands and runnable code:

1) Dataset & annotation:
  - A clear filesystem layout for images and labels (YOLO format), plus an example data.yaml.
  - Recommended image capture checklist and augmentation strategy (exact parameters for brightness, rotation, flip, color jitter).
  - Example script to convert images/annotations into the required YOLOv8 structure.

2) Training scripts:
  - A full `requirements.txt`.
  - Exact commands (or Python script) to train with Ultralytics YOLOv8 (yolov8n) including recommended hyperparameters (epochs, batch, imgsz, lr).
  - A small training wrapper to save the best checkpoint and produce metrics (mAP, precision, recall).

3) Export & optimization:
  - Commands to export trained weights to ONNX and TFLite.
  - A script to run post-training quantization (full integer if possible) and benchmark model size & inference time on CPU.
  - Guidance on representative dataset creation for quantization (example code).

4) Raspberry Pi inference:
  - A ready-to-run Python script for inference on Raspberry Pi 5 using either ONNX (OpenCV DNN) or TFLite runtime, including exact post-processing for YOLOv8 exported outputs.
  - A lightweight Flask or simple WebSocket web UI example showing live camera preview, detections, and a “Sort” manual override button.

5) Calibration & mapping:
  - A script to calibrate a top-down camera to map pixel coordinates to real-world coordinates (affine / homography).
  - Example calibration images and how to compute the transform matrix.

6) Arduino integration:
  - Arduino firmware (C++) that accepts serial commands from the Pi (e.g., `"ANGLE 90 60 120 95 150"` or `"MOVE X Y CLASS"`) and drives five servos (base, shoulder, elbow, wrist, gripper) smoothly with interpolation, distance sensing, and safety checks.
  - Example Pi-side function that converts detection bbox → world XY → IK joint angles → serial commands.

7) Testing & evaluation:
  - A test bench script that runs N inference cycles, logs detections, actions, and computes sorting accuracy and per-class precision/recall.
  - A short plan to run a 50-sample demo and compute metrics.

8) Troubleshooting & tips:
  - Common failure modes (lighting, ambiguous ripeness), and how to address them.
  - Recommended hardware parts with approximate specs (camera, servo torque, power supply).

Wrap everything in a zip with:
  - `train/` (scripts + config)
  - `export/` (export scripts)
  - `pi/` (inference scripts & requirements)
  - `arduino/` (firmware)
  - `docs/` (readme + steps to reproduce)
Make sure all code is runnable and documented inline.

Start by generating the `requirements.txt` and the `data.yaml` example, then produce the training script and the Pi inference script. Provide short command sequences for Colab GPU usage as a fallback.
